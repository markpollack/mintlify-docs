---
title: "Spring AI Bench"
description: "Open benchmarking suite for Java-centric AI developer agents"
---

<img src="https://img.shields.io/badge/Status-Incubating-blue" />

[GitHub](https://github.com/spring-ai-community/spring-ai-bench) â€¢ [Docs](https://spring-ai-community.github.io/spring-ai-bench/) â€¢ [Maven](https://central.sonatype.com/artifact/org.springaicommunity.bench/bench-core)

## Overview

Spring AI Bench measures modern agents on real enterprise development tasks â€” issue triage, PR review, coverage uplift, compliance validation, dependency upgrades. Run benchmarks on YOUR repos to measure YOUR scenarios.

**If agents have evolved, benchmarks must evolve too.**

<img src="/images/projects/spring-ai-bench-diagram.png" alt="Spring AI Bench Benchmarking" />

<Warning>
Existing benchmarks (SWE-bench) measure yesterday's agents on static 2023 Python patches. They can't evaluate the agents teams actually use (Claude, Gemini, Amazon Q, Amp) on enterprise Java workflows.
</Warning>

## Why Different from SWE-bench

<CardGroup cols={3}>
  <Card title="Full Dev Lifecycle" icon="circle-nodes">
    Beyond patch loops: triage, PR review, coverage, compliance
  </Card>
  <Card title="Java-First" icon="mug-hot">
    Agents show dismal 7-10% lower results on Java - we need better benchmarks
  </Card>
  <Card title="Any Agent" icon="wand-magic-sparkles">
    Claude, Gemini, Amazon Q, Amp, custom â€” not just one architecture
  </Card>
  <Card title="Reproducible" icon="repeat">
    One-click Docker + open scaffolding
  </Card>
  <Card title="Modern Paradigm" icon="forward">
    2025 declarative goal agents, not 2024 patch-loops
  </Card>
  <Card title="Open Standards" icon="handshake">
    Following best practices for benchmark design
  </Card>
</CardGroup>

## What Spring AI Bench Does

**Can AI act as a true Java developer agent?**

Not just fixing bugs, but:

<Steps>
  <Step title="Issue Analysis">
    Analyzing and labeling issues with domain-specific labels
  </Step>
  <Step title="PR Review">
    Comprehensive pull request analysis with risk assessment
  </Step>
  <Step title="Test Coverage">
    Raising coverage while keeping builds green
  </Step>
  <Step title="Static Analysis">
    Cleaning up checkstyle violations and code quality issues
  </Step>
  <Step title="API Migration">
    Migrating APIs and upgrading dependencies
  </Step>
  <Step title="Compliance">
    Keeping builds compliant with enterprise standards
  </Step>
</Steps>

That's the standard enterprise developers hold themselves to â€” and the standard we should evaluate AI against.

## Run It Yourself

Unlike static benchmarks, Spring AI Bench runs on YOUR repos:

```bash
# 1. Clone and build dependencies (5 minutes)
git clone https://github.com/spring-ai-community/spring-ai-agents.git
cd spring-ai-agents && ./mvnw clean install -DskipTests

git clone https://github.com/spring-ai-community/spring-ai-bench.git
cd spring-ai-bench

# 2. Set your API keys
export ANTHROPIC_API_KEY=your_key
export GEMINI_API_KEY=your_key

# 3. Run on YOUR codebase
./mvnw test -Dtest=HelloWorldMultiAgentTest -pl bench-agents

# 4. View results in your browser
open file:///tmp/bench-reports/index.html
```

## Current Implementation

### Supported Agent Providers

Spring AI Bench integrates with multiple AI agent providers through the Spring AI Agents framework:

<CardGroup cols={3}>
  <Card title="Claude Code" icon="robot">
    <img src="/images/agents/claude-code-logo.png" alt="Claude Code" style={{width: '60px', marginBottom: '0.5rem'}} />

    Anthropic's Claude via CLI integration
  </Card>
  <Card title="Gemini" icon="sparkles">
    <img src="/images/agents/gemini-logo.png" alt="Gemini" style={{width: '60px', marginBottom: '0.5rem'}} />

    Google's Gemini models
  </Card>
  <Card title="Amazon Q" icon="aws">
    <img src="/images/agents/amazon-q-logo.png" alt="Amazon Q" style={{width: '60px', marginBottom: '0.5rem'}} />

    AWS's AI development assistant
  </Card>
  <Card title="Amp" icon="wand-magic">
    <img src="/images/agents/amp-logo.png" alt="Amp" style={{width: '60px', marginBottom: '0.5rem'}} />

    Amp AI agent platform
  </Card>
  <Card title="Codex" icon="code">
    <img src="/images/agents/codex-logo.png" alt="Codex" style={{width: '60px', marginBottom: '0.5rem'}} />

    OpenAI Codex integration
  </Card>
  <Card title="Custom Agents" icon="puzzle-piece">
    Bring your own agent implementation
  </Card>
</CardGroup>

All agent providers support the same benchmark specifications, enabling fair comparisons across different AI models and platforms.

### Benchmark Tracks

<AccordionGroup>
  <Accordion title="âœ… Production Ready">
    - **hello-world**: File creation and basic infrastructure validation
    - **Code Coverage Uplift**: Autonomous test generation achieving 71.4% coverage on Spring tutorials
  </Accordion>

  <Accordion title="ðŸš§ In Active Development">
    - **Issue Analysis & Labeling**: Automated issue triage and classification
    - **Pull Request Review**: Comprehensive PR analysis with structured reports
    - **Static Analysis Remediation**: Fix code quality issues while preserving functionality
  </Accordion>

  <Accordion title="ðŸ“‹ Future Roadmap">
    - Integration Testing
    - Bug Fixing
    - Dependency Upgrades
    - API Migration
    - Compliance Validation
    - Performance Optimization
    - Documentation Generation
  </Accordion>
</AccordionGroup>

## Code Coverage Achievement

One of the most impressive demonstrations of Spring AI Bench is the **autonomous code coverage agent** that increased test coverage from **0% to 71.4%** on Spring's official [gs-rest-service](https://spring.io/guides/gs/rest-service) tutorial.

<CardGroup cols={2}>
  <Card title="Coverage Results" icon="chart-line">
    - **Starting Coverage**: 0%
    - **Final Coverage**: 71.4%
    - **Tests Generated**: Complete test suite
    - **Build Status**: âœ… All tests passing
  </Card>
  <Card title="Code Quality" icon="star">
    - **Claude**: Production-ready tests with @WebMvcTest, jsonPath(), BDD naming
    - **Gemini**: Same coverage, but used slower @SpringBootTest patterns
    - **Key Insight**: Model quality matters beyond just metrics
  </Card>
</CardGroup>

<Note>
**Learn More**: See the full [Code Coverage Analysis](https://spring-ai-community.github.io/spring-ai-bench/index.html#_code_coverage_agent) in the official documentation for detailed results and methodology.
</Note>

## Architecture

Spring AI Bench is built around a **Sandbox abstraction**:

<CardGroup cols={3}>
  <Card title="LocalSandbox" icon="laptop">
    Direct process execution (fast, development)
  </Card>
  <Card title="DockerSandbox" icon="docker">
    Container isolation (secure, production-ready)
  </Card>
  <Card title="CloudSandbox" icon="cloud">
    Distributed execution (planned)
  </Card>
</CardGroup>

**Key components:**
- **BenchHarness**: End-to-end benchmark execution
- **AgentRunner**: Agent execution with Spring AI Agents integration
- **SuccessVerifier**: Validation of benchmark results
- **ReportGenerator**: HTML and JSON report generation

## Resources

<CardGroup cols={2}>
  <Card
    title="Official Documentation"
    icon="book"
    href="https://spring-ai-community.github.io/spring-ai-bench/"
  >
    Complete documentation and analysis
  </Card>
  <Card
    title="GitHub Repository"
    icon="github"
    href="https://github.com/spring-ai-community/spring-ai-bench"
  >
    View source code and contribute
  </Card>
  <Card
    title="Getting Started"
    icon="rocket"
    href="https://spring-ai-community.github.io/spring-ai-bench/getting-started.html"
  >
    Quick start guide and setup
  </Card>
  <Card
    title="Code Coverage Results"
    icon="chart-line"
    href="https://spring-ai-community.github.io/spring-ai-bench/index.html#_code_coverage_agent"
  >
    Detailed coverage analysis and results
  </Card>
  <Card
    title="Agent Integration"
    icon="robot"
    href="https://spring-ai-community.github.io/spring-ai-bench/agents/claude-code.html"
  >
    Setting up AI agents for benchmarking
  </Card>
  <Card
    title="Why Different from SWE-bench"
    icon="magnifying-glass-chart"
    href="https://spring-ai-community.github.io/spring-ai-bench/#_the_evidence_why_swe_bench_falls_short"
  >
    Evidence and comparative analysis
  </Card>
</CardGroup>

## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](https://github.com/spring-ai-community/spring-ai-bench/blob/main/LICENSE) file for details.
